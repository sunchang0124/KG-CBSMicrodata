{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import os, time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_categories(base_url, category_head):\n",
    "    \n",
    "    ### Get all Microdata categories ###\n",
    "    categories_url = base_url+category_head\n",
    "    categories_response = requests.get(categories_url)\n",
    "\n",
    "    categories_soup = BeautifulSoup(categories_response.text)\n",
    "    categories_browse = categories_soup.findAll('a')\n",
    "    \n",
    "    categories_url_list = []\n",
    "    categories_list = []\n",
    "    for item in categories_browse:\n",
    "        try:\n",
    "            if category_head in item['href']:\n",
    "                categories_url_list.append(base_url+item['href'])\n",
    "                categories_list.append(item['href'].split('/')[-1])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return categories_url_list, categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_variables(variables_url, variable_base_url):\n",
    "\n",
    "    ### Crawl all variables names on the category web page ###\n",
    "\n",
    "    variables_response = requests.get(variables_url)\n",
    "\n",
    "    variables_soup = BeautifulSoup(variables_response.text)\n",
    "    variables_browse = variables_soup.findAll('a')\n",
    "\n",
    "    ### List all variables into a list ###\n",
    "    variables_list = []\n",
    "    for item in variables_browse:\n",
    "        try:\n",
    "            variable = item[\"data-id\"][:-2]\n",
    "            variables_list.append(variable)\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "    variable_url_list = []\n",
    "    for item in variables_list:\n",
    "        variable_url_list.append(variable_base_url+item)\n",
    "            \n",
    "    return variable_url_list, variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(variable_url_list, base_url):\n",
    "### Obtain Metadata phd file ###\n",
    "    time.sleep(1)\n",
    "    download_pages = []\n",
    "\n",
    "    for each_variable_url in variable_url_list:\n",
    "\n",
    "        each_variable_response = requests.get(each_variable_url)\n",
    "\n",
    "        each_variable_soup = BeautifulSoup(each_variable_response.text)\n",
    "        each_variable_browse = each_variable_soup.findAll('a')\n",
    "\n",
    "        for item in each_variable_browse:\n",
    "            try:\n",
    "                if \".pdf\" in item[\"href\"]:\n",
    "                    download_pages.append(base_url+item[\"href\"])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except:\n",
    "                print(\"This is not a KeyError!\")\n",
    "                \n",
    "    return download_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download PDF files ###\n",
    "def download_pdf(download_pages, each_category):\n",
    "    save_path_base = os.getcwd() + \"/MicrodataFiles/%s/\" %(each_category)\n",
    "    \n",
    "    # Check if folder exists #\n",
    "    create_new_folder(save_path_base)\n",
    "    \n",
    "    \n",
    "    for item in range(0, len(download_pages)):\n",
    "        extracted_file_name = download_pages[item].split('/')[-1]\n",
    "        file_name_to_save = save_path_base + extracted_file_name.split('?la=nl-nl')[0]\n",
    "        \n",
    "        ifExist = check_folder_exist(file_name_to_save)\n",
    "        if ifExist == \"is_a_file\":\n",
    "            print(\"%s has been downloaded!\" %extracted_file_name.split('?la=nl-nl')[0])\n",
    "        else:\n",
    "            try:\n",
    "                \n",
    "                urllib.request.urlretrieve(download_pages[item], file_name_to_save)\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                print(\"%s is NOT downloaded! Please try it again later.\" %extracted_file_name.split('?la=nl-nl')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder_exist(file_path):\n",
    "    \n",
    "    ifExist = False\n",
    "    # If this file object exist.\n",
    "    if(os.path.exists(file_path)):\n",
    "        \n",
    "        # If this is a file.\n",
    "        if(os.path.isfile(file_path)):\n",
    "            ifExist = \"is_a_file\"\n",
    "\n",
    "        # This is a directory.    \n",
    "        else:\n",
    "            ifExist = True\n",
    "            \n",
    "    else:\n",
    "        ifExist = False\n",
    "        \n",
    "    return ifExist\n",
    "\n",
    "def create_new_folder(file_path):\n",
    "    if(not check_folder_exist(file_path)):\n",
    "        os.mkdir(file_path)\n",
    "        print(file_path + \" has been created. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(): \n",
    "base_url = \"https://www.cbs.nl\"\n",
    "category_head = \"/nl-nl/onze-diensten/maatwerk-en-microdata/microdata-zelf-onderzoek-doen/catalogus-microdata/\"\n",
    "variable_base_url = \"https://www.cbs.nl/nl-nl/onze-diensten/maatwerk-en-microdata/microdata-zelf-onderzoek-doen/microdatabestanden/\"\n",
    "\n",
    "# Retrieve categories #\n",
    "categories_url_list, categories_list = retrieve_categories(base_url, category_head)\n",
    "print(\"There are %s categories\" %str(len(categories_url_list)))\n",
    "\n",
    "overview = []\n",
    "# Retrieve variables in each category #\n",
    "for category_item in range(0, len(categories_url_list)):\n",
    "\n",
    "    variable_url_list, variables_list = retrieve_variables(categories_url_list[category_item], variable_base_url)\n",
    "    print(\"In category (%s), there are %s variables\" %(categories_list[category_item], str(len(variable_url_list))))\n",
    "    overview.append(variables_list)\n",
    "    download_pages = download_page(variable_url_list, base_url)\n",
    "    \n",
    "    download_pdf(download_pages, categories_list[category_item])\n",
    "\n",
    "    print(\"****** Retrieval of %s is done! ******\" %(categories_list[category_item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "overview_dataFrame = pd.DataFrame(overview).transpose()\n",
    "overview_dataFrame.columns = categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_dataFrame.to_csv(\"overview.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
