{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -U nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import os, time\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_categories(base_url, category_head):\n",
    "    \n",
    "    ### Get all Microdata categories ###\n",
    "    categories_url = base_url+category_head\n",
    "    categories_response = requests.get(categories_url)\n",
    "\n",
    "    categories_soup = BeautifulSoup(categories_response.text)\n",
    "    categories_browse = categories_soup.findAll('a')\n",
    "    \n",
    "    categories_url_list = []\n",
    "    categories_list = []\n",
    "    for item in categories_browse:\n",
    "        try:\n",
    "            if category_head in item['href']:\n",
    "                categories_url_list.append(base_url+item['href'])\n",
    "                categories_list.append(item['href'].split('/')[-1])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return categories_url_list, categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_variables(variables_url, variable_base_url):\n",
    "\n",
    "    ### Crawl all variables names on the category web page ###\n",
    "\n",
    "    variables_response = requests.get(variables_url)\n",
    "\n",
    "    variables_soup = BeautifulSoup(variables_response.text)\n",
    "    variables_browse = variables_soup.findAll('a')\n",
    "\n",
    "    ### List all variables into a list ###\n",
    "    variables_list = []\n",
    "    for item in variables_browse:\n",
    "        try:\n",
    "            variable = item[\"data-id\"][:-2]\n",
    "            variables_list.append(variable)\n",
    "        except KeyError:\n",
    "            pass\n",
    "            \n",
    "    variable_url_list = []\n",
    "    for item in variables_list:\n",
    "        variable_url_list.append(variable_base_url+item)\n",
    "            \n",
    "    return variable_url_list, variables_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(variable_url_list, base_url):\n",
    "### Obtain Metadata phd file ###\n",
    "    time.sleep(1)\n",
    "    download_pages = []\n",
    "    landing_pages = []\n",
    "\n",
    "    for each_variable_url in variable_url_list:\n",
    "\n",
    "        each_variable_response = requests.get(each_variable_url)\n",
    "\n",
    "        each_variable_soup = BeautifulSoup(each_variable_response.text)\n",
    "        each_variable_browse = each_variable_soup.findAll('a')\n",
    "\n",
    "        for item in each_variable_browse:\n",
    "            try:\n",
    "                if \".pdf\" in item[\"href\"]:\n",
    "                    download_pages.append(base_url+item[\"href\"])\n",
    "                    landing_pages.append(each_variable_url)\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except:\n",
    "                print(\"This is not a KeyError!\")\n",
    "                \n",
    "    return download_pages, landing_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrievel landing and downloading path ###\n",
    "base_url = \"https://www.cbs.nl\"\n",
    "category_head = \"/nl-nl/onze-diensten/maatwerk-en-microdata/microdata-zelf-onderzoek-doen/catalogus-microdata/\"\n",
    "variable_base_url = \"https://www.cbs.nl/nl-nl/onze-diensten/maatwerk-en-microdata/microdata-zelf-onderzoek-doen/microdatabestanden/\"\n",
    "\n",
    "# Retrieve categories #\n",
    "categories_url_list, categories_list = retrieve_categories(base_url, category_head)\n",
    "print(\"There are %s categories\" %str(len(categories_url_list)))\n",
    "\n",
    "overview_landingPage = dict()\n",
    "overview_downloadPage = dict()\n",
    "# Retrieve variables in each category #\n",
    "for category_item in range(0, len(categories_url_list)):\n",
    "\n",
    "    variable_url_list, variables_list = retrieve_variables(categories_url_list[category_item], variable_base_url)\n",
    "    print(\"In category (%s), there are %s variables\" %(categories_list[category_item], str(len(variable_url_list))))\n",
    "#     overview.append(variables_list)\n",
    "    download_pages, landing_pages = download_page(variable_url_list, base_url)\n",
    "    overview_landingPage.update({categories_list[category_item]: landing_pages})\n",
    "    overview_downloadPage.update({categories_list[category_item]: download_pages})\n",
    "#     download_pdf(download_pages, categories_list[category_item])\n",
    "\n",
    "    print(\"****** Retrieval of %s is done! ******\" %(categories_list[category_item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df_EN = pd.read_csv(\"CBSMicrodataInfoEN.csv\")\n",
    "topics_df_NL = pd.read_csv(\"CBSMicrodataInfoNL_2.csv\")\n",
    "\n",
    "variables_df_EN = pd.read_csv(\"CBSMicrodataVariablesEN.csv\")\n",
    "topics_df_EN['File_id'] = topics_df_NL['File_id'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_df_EN\n",
    "downloadPage=[]\n",
    "landingPage=[]\n",
    "for i in range(0, len(topics_df_EN['File_id'])):\n",
    "    cnt_pre = len(downloadPage)\n",
    "    for c in overview_downloadPage.keys():\n",
    "        for n in range(0, len(overview_downloadPage[c])):\n",
    "            if topics_df_EN['File_id'][i] == overview_downloadPage[c][n].replace('?la=nl-nl','').split('/')[-1][:-4]:\n",
    "                if topics_df_EN['Category'][i] == c:\n",
    "                    downloadPage.append(overview_downloadPage[c][n])\n",
    "                    landingPage.append(overview_landingPage[c][n])\n",
    "                    \n",
    "    cnt_post = len(downloadPage)\n",
    "    if cnt_pre != cnt_post-1:\n",
    "        print(topics_df_EN['File_id'][i])\n",
    "        print(cnt_pre, cnt_post)\n",
    "        downloadPage.append(None)\n",
    "        landingPage.append(None)\n",
    "    \n",
    "topics_df_EN['Download_page'] = downloadPage\n",
    "topics_df_EN['Landing_page'] = landingPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df_combined = pd.read_csv(\"CBSMicrodataInfo_combined_v2.csv\")\n",
    "# topics_df_combined_eywordv3 = topics_df_combined.drop(['Keyword_1', 'Keyword_2', 'Keyword_3', 'Keyword_4', 'Keyword_5',\\\n",
    "#                  'K_6', 'Keyword_7','Keyword_8', 'Keyword_9'], axis=1)\n",
    "# topics_df_combined_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split DATE to YEAR MONTH DAY ###\n",
    "Year = []\n",
    "Month = []\n",
    "Day = []\n",
    "for i in topics_df_combined['Date_EN']:\n",
    "    try:\n",
    "        if int(i.split('-')[0])==0:\n",
    "            Day.append(None)\n",
    "        else:\n",
    "            Day.append(i.split('-')[0])\n",
    "        Month.append(i.split('-')[1])\n",
    "        Year.append(i.split('-')[2])\n",
    "    except:\n",
    "        Day.append(None)\n",
    "        Month.append(None)\n",
    "        Year.append(None)\n",
    "        \n",
    "topics_df_combined['Year']=Year\n",
    "topics_df_combined['Month']=Month\n",
    "topics_df_combined['Day']=Day\n",
    "\n",
    "topics_df_combined['Date_EN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords=[]\n",
    "for sentence in topics_df_combined['Title_EN']:\n",
    "    each_topic_keywords=[]\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "    except:\n",
    "        Keywords.append(None)\n",
    "    else:\n",
    "    \n",
    "        for i in tagged:\n",
    "            if i[1] == 'NN': #or i[1] == 'NNP':\n",
    "                if i[0] not in each_topic_keywords:\n",
    "                    each_topic_keywords.append(i[0])\n",
    "                \n",
    "        Keywords.append(each_topic_keywords)\n",
    "### Collect all words ###\n",
    "new_key=[]\n",
    "for i in Keywords:\n",
    "    try:\n",
    "        for j in i:\n",
    "            new_key.append(j)\n",
    "    except:\n",
    "        pass\n",
    "len(new_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take words which appear more than twice ###\n",
    "new_key_2=[]\n",
    "for i in Keywords:\n",
    "    each=[]\n",
    "    try:\n",
    "        for j in i:\n",
    "            if Counter(new_key)[j] > 2:\n",
    "                each.append(j)\n",
    "#             else:\n",
    "#                 each.append()\n",
    "    except:\n",
    "        each.append(None)\n",
    "    \n",
    "    new_key_2.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df = pd.DataFrame.from_records(new_key_2)\n",
    "key_df.columns = ['Keyword_1', 'Keyword_2', 'Keyword_3', 'Keyword_4', 'Keyword_5']\n",
    "key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Named Entity Recognition ###\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_keywords = []\n",
    "for i in topics_df_combined['Notes_EN']:\n",
    "    each_note = []\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(i)\n",
    "        for X in doc.ents:\n",
    "            if X.label_ == 'ORG' and X.text != 'CBS':\n",
    "                if X.text not in each_note:\n",
    "                    each_note.append(X.text)\n",
    "                \n",
    "            elif X.label_ =='DATE':\n",
    "                for item in X.text.split(' '):\n",
    "                    if item.isdigit():\n",
    "                        \n",
    "                        if (int(item) > 1900 and int(item) < 2500) or (int(item) > 10000):\n",
    "                            if item not in each_note:\n",
    "                                each_note.append(item)\n",
    "\n",
    "\n",
    "        notes_keywords.append(each_note)\n",
    "    \n",
    "    except:\n",
    "        notes_keywords.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2343"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes_keywords\n",
    "### Collect all words ###\n",
    "new_notes_keywords=[]\n",
    "for i in notes_keywords:\n",
    "    try:\n",
    "        for j in i:\n",
    "            new_notes_keywords.append(j)\n",
    "    except:\n",
    "        pass\n",
    "len(new_notes_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take words which appear more than twice ###\n",
    "new_notes_keywords_2=[]\n",
    "for i in notes_keywords:\n",
    "    each=[]\n",
    "    try:\n",
    "        for j in i:\n",
    "            if Counter(new_notes_keywords)[j] > 2:\n",
    "                if j not in [\"Research See\",\"the Version History\", \"Remote Access\", \"\\uf0b7\",\\\n",
    "                            \"Code_Listings\",\"The Version History\",\"RINPERSOON\", \"GBA\"]:\n",
    "                    if j == \"the Production Statistics (PS\" and \"Production Statistics\" not in each:\n",
    "                        each.append(\"Production Statistics\")\n",
    "                    elif j == \"SBS\" and \"the Structural Business Statistics\" not in each:\n",
    "                        each.append(\"the Structural Business Statistics\")\n",
    "                    else:\n",
    "                        if j not in each:\n",
    "                            each.append(j)\n",
    "                    \n",
    "              \n",
    "    except:\n",
    "        each.append(None)\n",
    "    \n",
    "    new_notes_keywords_2.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(new_notes_keywords).most_common()\n",
    "# len(notes_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_df_add = pd.DataFrame.from_records(new_notes_keywords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_key_df=[]\n",
    "for i in range(0, len(key_df)):\n",
    "    flag = False\n",
    "    new_row = []\n",
    "    for item in key_df.iloc[i]:\n",
    "        if item:\n",
    "            new_row.append(item)\n",
    "        if not item:\n",
    "            if not flag:\n",
    "                flag=True\n",
    "                for add_item in key_df_add.iloc[i]:\n",
    "                    if add_item:\n",
    "                        new_row.append('_'.join(word for word in add_item.split(' ')))\n",
    "    new_key_df.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_key_df=pd.DataFrame.from_records(new_key_df)\n",
    "new_key_df.columns = ['Keyword_1', 'Keyword_2', 'Keyword_3', 'Keyword_4', 'Keyword_5',\\\n",
    "                     'Keyword_6', 'Keyword_7', 'Keyword_8', 'Keyword_9', 'Keyword_10',\\\n",
    "                     'Keyword_11', 'Keyword_12' ,'Keyword_13', 'Keyword_14', 'Keyword_15', 'Keyword_16']\n",
    "# new_key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,17):\n",
    "    topics_df_combined_v3['Keyword_%s' %str(i)]=new_key_df['Keyword_%s' %str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df_combined_v3.to_csv('CBSMicrodataInfo_combined_v3.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_des=[]\n",
    "for i in variables_df_new['Variable_def_EN']:\n",
    "    try:\n",
    "        brief_des.append(i.split('.')[0])\n",
    "    except:\n",
    "        brief_des.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords=[]\n",
    "for sentence in brief_des:\n",
    "    each_topic_keywords=[]\n",
    "    try:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "    except:\n",
    "        Keywords.append(None)\n",
    "    else:\n",
    "    \n",
    "        for i in tagged:\n",
    "            if i[1] == 'NNS' or i[1] == 'NNP':\n",
    "                each_topic_keywords.append(i[0])\n",
    "        Keywords.append(each_topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
